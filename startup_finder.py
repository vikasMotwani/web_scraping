# -*- coding: utf-8 -*-
"""Startup_Finder.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M8krh1ER0rRLVy2HTr_FqYSctmCGUfcW
"""

#install and import librariess
#in terminal run: pip install -r requirements.txt

import requests
from bs4 import BeautifulSoup
import pandas as pd

#setup connection to apis
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',
    'Range': 'bytes=0-1024'
}

session = requests.Session()
companies=[]

def scrape(url):
  response = session.get(url, headers = headers).text
  soup = BeautifulSoup(response, 'html.parser')
  result = soup.find('div', {'class': 'infinite-container'})
  li =  result.find_all('div', {'class': 'infinite-item'})
  for i in li:
    c = i.find('a', {'id': 'startup-website-link'})
    if not c:
      continue
    company = { 'name': c.text , 'link': c['href'] }
    companies.append(company)
  return soup

def next_url(soup):
  pagination = soup.find('a', {'class': 'infinite-more-link'})
  if pagination and 'href' in pagination.attrs:
    return pagination['href']

def scraper(base_url, max_pages):
  soup = scrape(base_url)
  for i in range(1, max_pages):
    url = next_url(soup)
    if not url:
      break
    target_url = f'https://topstartups.io/{url}'
    soup = scrape(target_url)

if __name__ == '__main__':
  base_url = 'https://topstartups.io/?hq_location=USA'
  max_pages = 1
  scraper(base_url, max_pages)
  print(companies)
#nlp, tokenize and filter keywords

#sort and output results

#visualize