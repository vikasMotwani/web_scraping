# -*- coding: utf-8 -*-
"""Startup_Finder.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M8krh1ER0rRLVy2HTr_FqYSctmCGUfcW
"""

#install and import librariess
#in terminal run: pip install -r requirements.txt

from bs4 import BeautifulSoup
import pandas as pd
from selenium import webdriver
from selenium.common.exceptions import WebDriverException
import time
from urllib.parse import urljoin
import re

#setup connection to apis
options = webdriver.ChromeOptions()
options.add_argument("--headless=new")
options.add_argument('--ignore-certificate-errors')
options.add_argument('--ignore-ssl-errors')
driver = webdriver.Chrome(options=options)

companies=[]

def get_companies_url(url):
  driver.get(url)
  time.sleep(5)
  response=driver.page_source.encode('utf-8').strip()
  soup = BeautifulSoup(response, 'html.parser')
  result = soup.find('div', {'class': 'infinite-container'})
  li =  result.find_all('div', {'class': 'infinite-item'})
  for i in li:
    c = i.find('a', {'id': 'startup-website-link'}, text=True, href=True)
    if not c:
      continue
    company = { 'name': c.text , 'link': c['href'].replace('?utm_source=topstartups.io', '') }
    companies.append(company)
  return soup

def next_page(soup):
  pagination = soup.find('a', {'class': 'infinite-more-link'})
  if pagination and 'href' in pagination.attrs:
    return pagination['href']

def scraper(base_url, max_pages):
  soup = get_companies_url(base_url)
  for i in range(1, max_pages):
    url = next_page(soup)
    if not url:
      print(f'Only {i} pages exist')
      break
    target_url = f'https://topstartups.io/{url}'
    soup = get_companies_url(target_url)

def get_careers_href(company):
  try:
    driver.get(company)
    print(company)
    time.sleep(5)
    response=driver.page_source.encode('utf-8').strip()
    soup = BeautifulSoup(response, 'html.parser')
    career_link = soup.find('a', href=True, text='Careers')
    if career_link:
      career_href = urljoin(company, career_link['href'])
      return career_href
    return urljoin(company, '/careers')
  except WebDriverException as e:
    return None

if __name__ == '__main__':
  base_url = 'https://topstartups.io/?hq_location=USA'
  max_pages = 1
  scraper(base_url, max_pages)
  #companies_df = get_careers_page(companies)
  companies_df = pd.DataFrame(companies)
  companies_df['careers'] = companies_df['link'].map(get_careers_href)
  print(companies_df)
  companies_final = companies_df.dropna()
  companies_final.to_json('company_careers.json', orient='records', lines=True)